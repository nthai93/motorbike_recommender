# ============================================================
# üìò preprocess.py ‚Äì Ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n cho 2 h∆∞·ªõng TF-IDF & W2V
# ============================================================
import pandas as pd, re, unicodedata, os
from underthesea import word_tokenize

# --- Load d·ªØ li·ªáu ---
def load_data(path="data/data_motorbikes.xlsx"):
    df = pd.read_excel(path)
    df = df.dropna(subset=["Ti√™u ƒë·ªÅ", "M√¥ t·∫£ chi ti·∫øt"])
    print(f"‚úÖ ƒê·ªçc th√†nh c√¥ng {len(df)} d√≤ng t·ª´ {path}")
    return df

# --- B·ªè d·∫•u ---
def remove_accents(text):
    text = unicodedata.normalize("NFD", text)
    text = text.encode("ascii", "ignore").decode("utf-8")
    return str(text)

# --- Load dictionary ---
def load_dict(path):
    if not os.path.exists(path): return {}
    with open(path, "r", encoding="utf-8") as f:
        lines = [l.strip() for l in f if l.strip()]
    return dict(line.split(":") for line in lines if ":" in line)

# ============================================================
# 1Ô∏è‚É£ FULL CLEAN ‚Äì cho TF-IDF (chu·∫©n h√≥a m·∫°nh)
# ============================================================
def clean_text_full(text, stop_words, teen_dict, eng_dict, wrong_dict):
    if pd.isnull(text): return ""
    text = str(text).lower()

    # ‚úÖ Thay teen code, t·ª´ sai, t·ª´ English
    for w, c in {**wrong_dict, **teen_dict, **eng_dict}.items():
        text = re.sub(rf"\b{re.escape(w)}\b", c, text)

    # ‚úÖ T√°ch t·ª´ gi·ªØ c·ª•m nghƒ©a
    text = word_tokenize(text, format="text")

    # ‚úÖ B·ªè d·∫•u, k√Ω t·ª± ƒë·∫∑c bi·ªát
    text = remove_accents(text)
    text = re.sub(r"[^a-z0-9\s]", " ", text)
    text = re.sub(r"\s+", " ", text).strip()

    # ‚úÖ Lo·∫°i stopword
    tokens = [w for w in text.split() if w not in stop_words and len(w) > 1]
    return " ".join(tokens)

# ============================================================
# 2Ô∏è‚É£ LIGHT CLEAN ‚Äì cho Gensim + Word2Vec (gi·ªØ nghƒ©a)
# ============================================================
def clean_text_light(text):
    if pd.isnull(text): return ""
    text = str(text).lower()
    text = word_tokenize(text, format="text")      # gi·ªØ d·∫•u
    text = re.sub(r"[^a-zA-Z0-9√°√†·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√©√®·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá"
                  r"√≠√¨·ªâƒ©·ªã√≥√≤·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√∫√π·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±"
                  r"√Ω·ª≥·ª∑·ªπ·ªµƒë\s]", " ", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text

# ============================================================
# 3Ô∏è‚É£ Pipeline ti·ªÅn x·ª≠ l√Ω
# ============================================================
def preprocess_data(df, mode="light"):
    if mode == "full":
        stop_words = [w.strip() for w in open("files/vietnamese-stopwords.txt", "r", encoding="utf-8")] \
            if os.path.exists("files/vietnamese-stopwords.txt") else []
        teen_dict  = load_dict("files/teencode.txt")
        eng_dict   = load_dict("files/english-vnmese.txt")
        wrong_dict = load_dict("files/wrong-word.txt")
        func = lambda x: clean_text_full(x, stop_words, teen_dict, eng_dict, wrong_dict)
    else:
        func = clean_text_light

    text_cols = ["Ti√™u ƒë·ªÅ", "Th∆∞∆°ng hi·ªáu", "D√≤ng xe", "Lo·∫°i xe", "Dung t√≠ch xe", "M√¥ t·∫£ chi ti·∫øt"]
    for col in text_cols:
        df[col] = df[col].apply(func)

    df["content"] = df[text_cols].agg(" ".join, axis=1)
    print(f"‚úÖ Ho√†n t·∫•t ti·ªÅn x·ª≠ l√Ω ({mode.upper()} mode).")
    return df

# ============================================================
# 4Ô∏è‚É£ Ch·∫°y th·ª≠
# ============================================================
if __name__ == "__main__":
    df = load_data()
    # ‚öôÔ∏è Thay 'full' b·∫±ng 'light' t√πy m·ª•c ti√™u
    df = preprocess_data(df, mode="light")
    df.to_csv("data/motorbike_clean.csv", index=False)
    print("üíæ ƒê√£ l∆∞u data/motorbike_clean.csv")
